"""
Evaluates a classifier for the PLDs of referenced URLs according to their
political leaning, i.e., left or right.
Writes results as tuples (PLD, confidence score) into two separate CSV-files.

Example call: python -m evaluation 'leaning_guesser' '../../../input_data/test_tweets.csv' '../../../output_data/left_tweets.csv' '../../../output_data/right_tweets.csv' -v
"""

import numpy as np
import sys
import torch
from argparse import ArgumentParser
from torchtext.data.utils import get_tokenizer

from data_file_handler import read_hists_from_file, read_tweets_from_csv, \
                              read_model_from_files, write_results_to_csv
from data_preprocessor import build_dataloader, preprocess_emos, preprocess_tags
from helpers import plot_acc_and_loss, print_log
from pld_dataset import PLDDataset

def apply_classifier(classifier, test_ldr):
    """ Performs a forward pass with 'classifier' per sample from 'test_ldr'.
    Returns predictions as list of lists. """
    predictions = []
    classifier.eval()
    for _, emos, tags, offsets in test_ldr:
        predictions += classifier(emos, tags, offsets, probs=True).tolist()
    return predictions

def evaluate_predictions(predictions):
    """ Calculates classes and confidences from 'predictions'. """
    classes = np.argmax(predictions, axis=1)
    confidences = np.amax(predictions, axis=1)
    return classes, confidences

# Main

def parse_arguments(args):
    """ Creates an ArgumentParser with help messages. """
    info =  """ Evaluation for a classifier for PLD media bias classification.
            Uses tweets from CSV-files generated by 'data_retriever.py'. """
    parser = ArgumentParser(description=info)
    parser.add_argument('cls', help="specify relative path to classifier")
    parser.add_argument('data', help="specify relative path to test data")
    parser.add_argument('res_l', help="specify relative path to left results")
    parser.add_argument('res_r', help="specify relative path to right results")
    parser.add_argument('-p', '--plot', action='store_true', default=False,
                        help="plot evaluation metrics from training")
    parser.add_argument('-v', '--verbose', action='store_true', default=False,
                        help="activate output")

    if len(args) < 1:  # show help, if no arguments are given
        parser.print_help(sys.stderr)
        sys.exit()
    return parser.parse_args(args)

def main(args):
    parsed_args = parse_arguments(args)

    tokenizer = get_tokenizer('basic_english')
    classifier, vocab = read_model_from_files(parsed_args.cls)
    print_log("Classifier and vocab loaded.", parsed_args.verbose)

    pld_ls, _, emos_pos_ls, emos_neg_ls, tags_ls, _ \
        = read_tweets_from_csv(parsed_args.data, parsed_args.verbose)

    # labels are not present for testing, set them to -1
    label_arr = np.full_like(pld_ls, fill_value=-1, dtype=int).tolist()
    emos_arr = preprocess_emos(emos_pos_ls, emos_neg_ls)
    tags_str_arr = preprocess_tags(tags_ls)

    pld_testset = PLDDataset(label_arr, emos_arr, tags_str_arr)
    pld_test_ldr = build_dataloader(pld_testset, 5, vocab, tokenizer)
    print_log("Test data loaded.", parsed_args.verbose)

    predictions = apply_classifier(classifier, pld_test_ldr)
    class_ls, confidence_ls = evaluate_predictions(predictions)
    write_results_to_csv(pld_ls, class_ls, confidence_ls, parsed_args.res_l,
                         parsed_args.res_r)
    print_log("Evaluation done and results saved.", parsed_args.verbose)

    if parsed_args.plot:
        trn_hist, val_hist = read_hists_from_file(parsed_args.cls)
        plot_acc_and_loss(trn_hist, val_hist)

if __name__ == '__main__':
    main(sys.argv[1:])
