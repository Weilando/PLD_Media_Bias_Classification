"""
Trains a classifier for the PLDs of referenced URLs according to their
political leaning, i.e., left or right.

Example call: python -m pld_classifier_trainer 'leaning_guesser' '../../../input_data/left_tweets.csv' '../../../input_data/right_tweets.csv' -b 30 -l 0.01 -e 12 -v
"""

import numpy as np
import sys
import torch
from argparse import ArgumentParser
from torch.optim import Adam

from data_file_handler import read_tweets_from_csv, write_hists_to_file, \
                              write_model_to_files
from data_preprocessor import build_vocab, build_dataloader
from helpers import print_log, gen_stat_msg, plot_acc_and_loss
from pld_classifier import build_classifier
from pld_dataset import PLDDataset, build_emos_arr, build_label_arr, \
                        build_tags_str_arr, split_dataset

# Trainer

def train_classifier(classifier, trn_ldr, val_ldr, ep=5, lr=0.01, verbose=True):
    """ Trains 'classifier' for 'ep' epochs using Adam with learning rate 'lr'.
    Uses data from 'trn_ldr' for training and 'val_ldr' for validation. Returns
    the trained 'classifier' and evaluation metrics from training. """
    opt = Adam(classifier.parameters(), lr=lr)
    trn_hist, val_hist = [], [] # save tuples (accuracy, loss)

    for e in range(1, ep+1):
        total_ok, total_count = 0, 0
        epoch_losses = []
        classifier.train()
        for labels, emos, tags, offsets in trn_ldr:
            # forward pass
            opt.zero_grad()
            predicted_labels = classifier(emos, tags, offsets)
            loss = classifier.loss_fct(predicted_labels, labels)

            # backward pass
            loss.backward()
            opt.step()

            # evaluation metrics
            epoch_losses.append(loss.item())
            total_ok += (predicted_labels.argmax(1) == labels).sum().item()
            total_count += labels.size(0)

        trn_hist.append((total_ok / total_count, np.mean(epoch_losses)))
        val_hist.append(calc_acc_and_loss(classifier, val_ldr))
        print_log(gen_stat_msg(e, trn_hist, val_hist), verbose)

    return classifier, np.array(trn_hist), np.array(val_hist)

def calc_acc_and_loss(classifier, ldr):
    """ Calculates the accuracy and loss of 'classifier' on 'ldr'. """
    classifier.eval()
    total_ok, total_count = 0, 0
    losses = []

    with torch.no_grad():
        for labels, emos, tags, offsets in ldr:
            predicted_labels = classifier(emos, tags, offsets)
            losses.append(classifier.loss_fct(predicted_labels, labels))
            # count correct classifications and samples
            total_ok += (predicted_labels.argmax(1) == labels).sum().item()
            total_count += labels.size(0)

    return total_ok / total_count, np.mean(losses) # return acc and loss

# Main

def parse_arguments(args):
    """ Creates an ArgumentParser with help messages. """
    info =  """ Trainer for a classifier for PLD media bias classification.
            Uses tweets from CSV-files generated by 'data_retriever.py'. """
    parser = ArgumentParser(description=info)
    parser.add_argument('model_name', help="specify name of trained classifier")
    parser.add_argument('data_l',
                        help="specify relative path to left training data")
    parser.add_argument('data_r',
                        help="specify relative path to right training data")
    parser.add_argument('-b', '--ba', type=int, default=43, metavar='N',
                        help="specify number of samples per batch")
    parser.add_argument('-e', '--ep', type=int, default=10, metavar='N',
                        help="specify number of epochs for training")
    parser.add_argument('-l', '--lr', type=float, default=0.01, metavar='R',
                        help="specify learning rate")
    parser.add_argument('-v', '--verbose', action='store_true', default=False,
                        help="activate output")

    if len(args) < 1:  # show help, if no arguments are given
        parser.print_help(sys.stderr)
        sys.exit()
    return parser.parse_args(args)

def main(args):
    parsed_args = parse_arguments(args)
    assert parsed_args.ba > 0
    assert parsed_args.ep > 0
    assert parsed_args.lr > 0.0

    l_pld_ls, l_cnts, l_emos_pos_ls, l_emos_neg_ls, l_tags_ls, _ \
        = read_tweets_from_csv(parsed_args.data_l, parsed_args.verbose)
    r_pld_ls, r_cnts, r_emos_pos_ls, r_emos_neg_ls, r_tags_ls, _ \
        = read_tweets_from_csv(parsed_args.data_r, parsed_args.verbose)

    label_arr = build_label_arr(len(l_pld_ls), len(r_pld_ls))
    emos_arr = build_emos_arr(l_emos_pos_ls, l_emos_neg_ls,
                              r_emos_pos_ls, r_emos_neg_ls, l_cnts, r_cnts)
    tags_str_arr = build_tags_str_arr(l_tags_ls, r_tags_ls)

    pld_dataset = PLDDataset(label_arr, emos_arr, tags_str_arr)
    vocab, tokenizer = build_vocab(tags_str_arr)
    print_log("Pre-processing done.", parsed_args.verbose)

    trn_set, val_set = split_dataset(pld_dataset, parsed_args.ba,
                                     verbose=parsed_args.verbose)
    trn_ldr = build_dataloader(trn_set, parsed_args.ba, vocab, tokenizer)
    val_ldr = build_dataloader(val_set, parsed_args.ba, vocab, tokenizer)

    classifier = build_classifier(vocab)
    classifier, trn_hist, val_hist = train_classifier(classifier, trn_ldr,
                                        val_ldr, parsed_args.ep,
                                        parsed_args.lr, parsed_args.verbose)
    write_model_to_files(parsed_args.model_name, classifier, vocab)
    write_hists_to_file(parsed_args.model_name, trn_hist, val_hist)
    print_log("Classifier trained. State and vocab saved.", parsed_args.verbose)
    if parsed_args.verbose:
        plot_acc_and_loss(trn_hist, val_hist)

if __name__ == '__main__':
    main(sys.argv[1:])
