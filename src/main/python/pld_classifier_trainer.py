"""
Trains a classifier for the PLDs of referenced URLs according to their
political leaning, i.e., left or right.

Example call: python -m pld_classifier_trainer 'leaning_guesser' '../../../input_data/left_tweets.csv' '../../../input_data/right_tweets.csv' -b 30 -l 0.01 -e 12 -v
"""

import numpy as np
import sys
import torch
from argparse import ArgumentParser
from torch.optim import Adam

from data_file_handler import read_tweets_from_csv
from data_preprocessor import build_vocab, build_dataloader
from helpers import print_log
from pld_classifier import PLDClassifier, PLDClassifierParam
from pld_dataset import PLDDataset, build_label_arr, build_emos_arr, \
                        build_tags_str_arr, split_dataset

# Trainer

def build_classifier(vocab):
    """ Builds a PLDClassifier with pretrained embedding from 'vocab'. """
    param = PLDClassifierParam() # standard parameter
    return PLDClassifier(param, embedding_weight=vocab.vectors)

def train_classifier(classifier, trn_ldr, val_ldr, ep=5, lr=0.01, verbose=True):
    """ Train 'classifier' for 'ep' epochs using Adam with learning rate 'lr'.
    Uses data from 'trn_ldr' for training and 'val_ldr' for validation. """
    opt = Adam(classifier.parameters(), lr=lr)

    for e in range(1, ep+1):
        epoch_losses = []
        classifier.train()
        for labels, emos, tags, offsets in trn_ldr:
            opt.zero_grad()
            prediction = classifier(emos, tags, offsets)
            loss = classifier.loss_fct(prediction, labels)

            loss.backward()
            opt.step()
            epoch_losses.append(loss.item())

        if verbose:
            trn_loss = np.mean(epoch_losses)
            acc = calc_val_acc(classifier, val_ldr)
            print(f"Epoch {e:2d}: train-loss={trn_loss:.3f}, val-acc={acc:.3f}")

    return classifier

def calc_val_acc(classifier, val_ldr):
    """ Calculates the validation accuracy of 'classifier' on 'val_ldr'. """
    classifier.eval()
    total_acc, total_count = 0, 0

    with torch.no_grad():
        for labels, emos, tags, offsets in val_ldr:
            predited_labels = classifier(emos, tags, offsets)
            loss = classifier.loss_fct(predited_labels, labels)
            total_acc += (predited_labels.argmax(1) == labels).sum().item()
            total_count += labels.size(0)
    return total_acc / total_count

# Main

def parse_arguments(args):
    """ Creates an ArgumentParser with help messages. """
    info =  """ Trainer for a classifier for PLD media bias classification.
            Uses tweets from CSV-files generated by 'data_retriever.py'. """
    parser = ArgumentParser(description=info)
    parser.add_argument('model_name', help="specify name of trained classifier")
    parser.add_argument('data_l',
                        help="specify relative path to left training data")
    parser.add_argument('data_r',
                        help="specify relative path to right training data")
    parser.add_argument('-b', '--ba', type=int, default=43, metavar='N',
                        help="specify number of samples per batch")
    parser.add_argument('-e', '--ep', type=int, default=10, metavar='N',
                        help="specify number of epochs for training")
    parser.add_argument('-l', '--lr', type=float, default=0.01, metavar='R',
                        help="specify learning rate")
    parser.add_argument('-v', '--verbose', action='store_true', default=False,
                        help="activate output")

    if len(args) < 1:  # show help, if no arguments are given
        parser.print_help(sys.stderr)
        sys.exit()
    return parser.parse_args(args)

def main(args):
    parsed_args = parse_arguments(args)
    assert parsed_args.ba > 0
    assert parsed_args.ep > 0
    assert parsed_args.lr > 0.0

    l_pld_ls, _, l_emos_pos_ls, l_emos_neg_ls, l_tags_ls, _ \
        = read_tweets_from_csv(parsed_args.data_l, parsed_args.verbose)
    r_pld_ls, _, r_emos_pos_ls, r_emos_neg_ls, r_tags_ls, _ \
        = read_tweets_from_csv(parsed_args.data_r, parsed_args.verbose)

    label_arr = build_label_arr(len(l_pld_ls), len(r_pld_ls))
    emos_arr = build_emos_arr(l_emos_pos_ls, l_emos_neg_ls,
                              r_emos_pos_ls, r_emos_neg_ls)
    tags_str_arr = build_tags_str_arr(l_tags_ls, r_tags_ls)

    pld_dataset = PLDDataset(label_arr, emos_arr, tags_str_arr)
    vocab, tokenizer = build_vocab(tags_str_arr)
    print_log("Pre-processing done.", parsed_args.verbose)

    trn_set, val_set = split_dataset(pld_dataset, parsed_args.ba,
                                     verbose=parsed_args.verbose)
    trn_ldr = build_dataloader(trn_set, parsed_args.ba, vocab, tokenizer)
    val_ldr = build_dataloader(val_set, parsed_args.ba, vocab, tokenizer)

    classifier = build_classifier(vocab)
    classifier = train_classifier(classifier, trn_ldr, val_ldr, parsed_args.ep,
                                  parsed_args.lr, parsed_args.verbose)
    torch.save(classifier.state_dict(), f"{parsed_args.model_name}.pt")
    torch.save(vocab, f"{parsed_args.model_name}_vocab.pt")
    print_log("Classifier trained. State and vocab saved.", parsed_args.verbose)

if __name__ == '__main__':
    main(sys.argv[1:])
